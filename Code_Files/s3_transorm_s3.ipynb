{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "import subprocess\n",
    "import re \n",
    "from io import StringIO \n",
    "from datetime import date, timedelta, datetime \n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "#!conda install -c conda-forge pygrib -n python3 -y\n",
    "import pygrib\n",
    "\n",
    "#!conda install h5py -n python3 -y\n",
    "import h5py\n",
    "\n",
    "#!conda install -c ostrokach gzip -n python3 -y\n",
    "import gzip\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which returns the total number of figures in a float, and the total number of decimal figures\n",
    "def precision_and_scale(x):\n",
    "    max_digits = 14\n",
    "    int_part = int(abs(x))\n",
    "    magnitude = 1 if int_part == 0 else int(math.log10(int_part)) + 1\n",
    "    if magnitude >= max_digits:\n",
    "        return (magnitude, 0)\n",
    "    frac_part = abs(x) - int_part\n",
    "    multiplier = 10 ** (max_digits - magnitude)\n",
    "    frac_digits = multiplier + int(multiplier * frac_part + 0.5)\n",
    "    while frac_digits % 10 == 0:\n",
    "        frac_digits /= 10\n",
    "    scale = int(math.log10(frac_digits))\n",
    "    return (magnitude + scale, scale) \n",
    "\n",
    "#function to transfer a file from s3 to efs\n",
    "def s3_to_efs(d, model, hour, field, forecast):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  grb.values.flatten()\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    commande_rm = 'rm ' + path_grb2\n",
    "    os.system(commande_rm)\n",
    "    path_save = '../efs/' + path_grb2\n",
    "    np.save(path_save,mat_res)\n",
    "    return(0)\n",
    "\n",
    "#function to download a file from s3, transfotm it and putting it back to s3\n",
    "def s3_csvgz_s3(d, model, hour, field, forecast):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  grb.values.flatten()\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    df = pd.DataFrame(mat_res)\n",
    "    name_csv = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.csv'\n",
    "    print(name_csv) \n",
    "    path_csv = '../efs/' + name_csv\n",
    "    df.to_csv(path_csv, index=False)\n",
    "    print('csv_fini')\n",
    "    with open(path_csv, 'rb') as f_in, gzip.open(path_csv +  '.gz', 'wb') as f_out:\n",
    "        f_out.writelines(f_in) \n",
    "    name_csv_gz = name_csv + '.gz'\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    s3 = boto3.client('s3')\n",
    "    key_data = 'remi/' + name_csv_gz\n",
    "    s3.upload_file(path_csv + '.gz' , bucket_name, key_data)\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    commande_rm_csv = 'rm ' + path_csv\n",
    "    os.system(commande_rm_csv)  \n",
    "    commande_rm_csvgz = 'rm ' + path_csv + '.gz'\n",
    "    os.system(commande_rm_csvgz) \n",
    "    return(0)\n",
    "\n",
    "#function to download a file from s3, transfotm it and putting it back to s3\n",
    "def s3_hdf5_s3(d, model, hour, field, forecast):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  grb.values.flatten()\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    name_hdf5 = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.h5'\n",
    "    path_hdf5 = '../efs/' + name_hdf5\n",
    "    h5f = h5py.File(path_hdf5, 'w')\n",
    "    h5f.create_dataset(name_hdf5, data=mat_res)\n",
    "    h5f.close() \n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    s3 = boto3.client('s3')\n",
    "    key_data = 'remi/' + name_hdf5\n",
    "    data = open(path_hdf5,'rb')\n",
    "    s3.upload_file(path_hdf5, bucket_name, key_data)\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    commande_rm_hdf5 = 'rm ' + path_hdf5\n",
    "    os.system(commande_rm_hdf5)  \n",
    "    return(0)\n",
    "\n",
    "#function to download a file from s3, transfotm it and putting it back to s3\n",
    "def s3_hdf5gz_s3(d, model, hour, field, forecast):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  grb.values.flatten()\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    mat_res[np.where(mat_res==9.969209968386869e+36)] = 0\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    name_hdf5gz = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.h5.gz'\n",
    "    path_hdf5gz = '../efs/' + name_hdf5gz\n",
    "    h5f = h5py.File(path_hdf5gz, 'w')\n",
    "    h5f.create_dataset(name_hdf5gz, data=mat_res, compression=\"gzip\")\n",
    "    h5f.close() \n",
    "    print(name_hdf5gz)\n",
    "    print(path_hdf5gz)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    s3 = boto3.client('s3')\n",
    "    key_data = 'remi/' + name_hdf5gz\n",
    "    data = open(path_hdf5gz,'rb')\n",
    "    s3.upload_file(path_hdf5gz, bucket_name, key_data)\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    commande_rm_hdf5 = 'rm ' + path_hdf5gz\n",
    "    os.system(commande_rm_hdf5)  \n",
    "    return(0)\n",
    "\n",
    "#function to download a file from s3, transfotm it and putting it back to s3\n",
    "def s3_npzcompressed_s3(d, model, hour, field, forecast):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  grb.values.flatten()\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    name_npz = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.npz'\n",
    "    path_npz = '../efs/' + name_npz\n",
    "    np.savez_compressed(path_npz, mat_res)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    s3 = boto3.client('s3')\n",
    "    key_data = 'remi/' + name_npz\n",
    "    data = open(path_npz,'rb')\n",
    "    s3.upload_file(path_npz, bucket_name, key_data)\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    commande_rm_npz = 'rm ' + path_npz\n",
    "    os.system(commande_rm_npz)  \n",
    "    return(0)\n",
    "\n",
    "#function to download a file from s3, transfotm it and putting it back to s3\n",
    "def s3_npy_s3(d, model, hour, field, forecast):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  grb.values.flatten()\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    name_npy = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.npy'\n",
    "    path_npy = '../efs/' + name_npy\n",
    "    np.save(path_npy, mat_res)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    s3 = boto3.client('s3')\n",
    "    key_data = 'remi/' + name_npy\n",
    "    data = open(path_npy,'rb')\n",
    "    s3.upload_file(path_npy, bucket_name, key_data)\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    commande_rm_npy = 'rm ' + path_npy\n",
    "    os.system(commande_rm_npy)  \n",
    "    return(0)\n",
    "\n",
    "def s3_hdf5gztrunc_s3(d, model, hour, field, forecast, round_number):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  np.round(grb.values.flatten(),round_number)\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    mat_res[np.where(mat_res==9.969209968386869e+36)] = 0\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    name_hdf5gz = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.h5.gz'\n",
    "    path_hdf5gz = '../efs/' + name_hdf5gz\n",
    "    h5f = h5py.File(path_hdf5gz, 'w')\n",
    "    h5f.create_dataset(name_hdf5gz, data=mat_res, compression=\"gzip\")\n",
    "    h5f.close() \n",
    "    print(name_hdf5gz)\n",
    "    print(path_hdf5gz)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    s3 = boto3.client('s3')\n",
    "    key_data = 'remi/' + name_hdf5gz\n",
    "    data = open(path_hdf5gz,'rb')\n",
    "    s3.upload_file(path_hdf5gz, bucket_name, key_data)\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    commande_rm_hdf5 = 'rm ' + path_hdf5gz\n",
    "    os.system(commande_rm_hdf5)  \n",
    "    return(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load a grib2 file from s3 and putting it back as a hdf5.gz file\n",
    "def s3_hdf5gztrunc_s3(d, model, hour, field, forecast, round_number):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  np.round(grb.values.flatten(),round_number)\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    mat_res[np.where( (mat_res==9.969209968386869e+36)  |  (mat_res==float('Inf')) )] = 0\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    name_hdf5gz = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.h5.gz'\n",
    "    path_hdf5gz = '../efs/' + name_hdf5gz\n",
    "    h5f = h5py.File(path_hdf5gz, 'w')\n",
    "    h5f.create_dataset(name_hdf5gz, data=mat_res, compression=\"gzip\")\n",
    "    h5f.close() \n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    s3 = boto3.client('s3')\n",
    "    key_data = 'remi/' + name_hdf5gz\n",
    "    data = open(path_hdf5gz,'rb')\n",
    "    s3.upload_file(path_hdf5gz, bucket_name, key_data)\n",
    "    commande_rm_hdf5gz = 'rm ' + path_hdf5gz\n",
    "    os.system(commande_rm_hdf5gz)  \n",
    "    return(0)\n",
    "\n",
    "def s3_hdf5gztruncrand_s3(d, model, hour, field, forecast, round_number):\n",
    "    start = time.time()\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  np.round(grb.values.flatten(),round_number)\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    mat_res[np.where( (mat_res==9.969209968386869e+36)  |  (mat_res==float('Inf')) )] = 0\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    name_hdf5gz = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.h5.gz'\n",
    "    path_hdf5gz = '../efs/' + name_hdf5gz\n",
    "    h5f = h5py.File(path_hdf5gz, 'w')\n",
    "    h5f.create_dataset(name_hdf5gz, data=mat_res, compression=\"gzip\")\n",
    "    h5f.close() \n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    s3 = boto3.client('s3')\n",
    "    tirage = random.random()\n",
    "    if (tirage<=0.8):\n",
    "        key_data = 'remi/train/' + name_hdf5gz\n",
    "    else:\n",
    "        key_data = 'remi/test/' + name_hdf5gz\n",
    "    data = open(path_hdf5gz,'rb')\n",
    "    s3.upload_file(path_hdf5gz, bucket_name, key_data)\n",
    "    commande_rm_hdf5gz = 'rm ' + path_hdf5gz\n",
    "    os.system(commande_rm_hdf5gz)  \n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test run time for the function s3_hdf5gztruncrand_s3\n",
    "hours = np.arange(24)\n",
    "fxx = [0]\n",
    "field = 'prs'\n",
    "model = 'hrrr'\n",
    "sDATE = date(2017, 1, 1)          # Start date\n",
    "eDATE = date(2017, 12, 31)          # End date (exclusive)\n",
    "days = (eDATE-sDATE).days\n",
    "DATES = [sDATE + timedelta(days=d) for d in range(days)]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "s3_hdf5gztruncrand_s3(DATES[0], model, hours[0], field, fxx[0], 4)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-processing\n",
    "# hours = np.arange(24)\n",
    "# fxx = [0]\n",
    "# field = 'prs'\n",
    "# model = 'hrrr'\n",
    "# sDATE = date(2017, 1, 1)          # Start date\n",
    "# eDATE = date(2017, 12, 31)          # End date (exclusive)\n",
    "# days = (eDATE-sDATE).days\n",
    "# DATES = [sDATE + timedelta(days=d) for d in range(days)]\n",
    "\n",
    "# DATES_try = DATES[0]\n",
    "# hours_try = hours[0:12]\n",
    "\n",
    "\n",
    "# n_proc = multiprocessing.cpu_count()\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# pool = Pool(processes=n_proc-1)\n",
    "# result = [ pool.apply_async(s3_hdf5gztrunc_s3, args=(DATES_try,model,h,field,fxx[0],4) ) for h in hours_try]\n",
    "\n",
    "# for i in range(len(result)):\n",
    "#     print(result[i].get())\n",
    "    \n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 2, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final execution chunk\n",
    "def s3_hdf5gztruncrand_s3(d, model, hour, field, forecast, round_number):\n",
    "    path_grb2 = \"%s_%s.t%02dz.wrf%sf%02d.grib2\" % (d.strftime('%Y%m%d'), model, hour, field, forecast)\n",
    "    bucket_name = 'jedi-pub'    \n",
    "    key_data = 'remi/' + path_grb2\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).download_file(key_data, path_grb2)\n",
    "    mat_res = np.zeros((1905141,686))\n",
    "    grbs = pygrib.open(path_grb2)\n",
    "    for j in range(684):\n",
    "        grb = grbs.readline()\n",
    "        mat_res[:,j] =  np.round(grb.values.flatten(),round_number)\n",
    "    lats, lons = grb.latlons()    \n",
    "    mat_res[:,684] = lats.flatten()\n",
    "    mat_res[:,685] = lons.flatten()\n",
    "    mat_res[np.where( (mat_res==9.969209968386869e+36)  |  (mat_res==float('Inf')) )] = 0\n",
    "    commande_rm_grib2 = 'rm ' + path_grb2\n",
    "    os.system(commande_rm_grib2)\n",
    "    name_hdf5gz = \"%s_%s.t%02dz.wrf%sf%02d\" % (d.strftime('%Y%m%d'), model, hour, field, forecast) + '.h5.gz'\n",
    "    path_hdf5gz = '../efs/' + name_hdf5gz\n",
    "    h5f = h5py.File(path_hdf5gz, 'w')\n",
    "    h5f.create_dataset(name_hdf5gz, data=mat_res, compression=\"gzip\")\n",
    "    h5f.close() \n",
    "    s3 = boto3.client('s3')\n",
    "    tirage = random.random()\n",
    "    if (tirage<=0.8):\n",
    "        key_data = 'remi/train/' + name_hdf5gz\n",
    "    else:\n",
    "        key_data = 'remi/test/' + name_hdf5gz\n",
    "    data = open(path_hdf5gz,'rb')\n",
    "    s3.upload_file(path_hdf5gz, bucket_name, key_data)\n",
    "    commande_rm_hdf5gz = 'rm ' + path_hdf5gz\n",
    "    os.system(commande_rm_hdf5gz)  \n",
    "    return(0)\n",
    "\n",
    "\n",
    "#Multi-processing\n",
    "hours = np.arange(24)\n",
    "fxx = [0]\n",
    "field = 'prs'\n",
    "model = 'hrrr'\n",
    "sDATE = date(2017, 1, 1)          # Start date\n",
    "eDATE = date(2017, 12, 31)          # End date (exclusive)\n",
    "days = (eDATE-sDATE).days\n",
    "DATES = [sDATE + timedelta(days=d) for d in range(days)]\n",
    "\n",
    "all_DATES_hours = [(d,h) for d in DATES for h in hours]\n",
    "for i in range(16,24):\n",
    "    all_DATES_hours.remove((date(2017, 2, 28),i))\n",
    "all_DATES_hours.remove((date(2017, 8, 29),1))   \n",
    "\n",
    "\n",
    "n_proc = multiprocessing.cpu_count()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pool = Pool(processes=n_proc-1)\n",
    "result = [ pool.apply_async(s3_hdf5gztrunc_s3, args=(d,model,h,field,fxx[0],4) ) for d,h in all_DATES_hours]\n",
    "\n",
    "for i in range(len(result)):\n",
    "    print(result[i].get())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
